### Context Driven Knowledge Protocols

The advent of Large Language Models (LLMs) has been fallen short from impressive. These AI models can achieve impressive feats
on text generation, summarization, structuring data, debug and write code and also (more basic) problem solving skills. Current state
of the art LLMs have been trained on large textual corpus (potentially close to all internet content) as well as fine tuned and later
improved using advanced techniques such as Reinforcement Learning from Human Feedback (RLHF). It is important to note, however, that
the majority of the LLMs capacities are direct consequence of the fact that many problems and contexts can be naturally expressed in
natural language. 

From empirical experience, one notices that LLMs are particularly good at generating structured data, as opposed to unstructured data. For
example, even though state of the art LLMs text generation capacities are impressive, these models tend to condense important information
and usually they are not capable to generate large quantities of technical expertise textual corpus.

Moreover, LLMs are not necessarily knowledgeable artifacts. Instead, we proclaim that LLMs are akin to generators (in usual programming 
language terms), while following extremely complex probabilitic densifty functions (PDFs). This analogy shows that LLMs are objects
that, by its very nature, are difficult to protocolize, that is, to build protocols on top of it, like decentralized networks. This
issue can be, possibly, minimized by fine tuned over different sets of specific tasks. However, it is difficult to coordinate multiple
LLMs operating on very distinct tasks. 

Moreover, LLMs, due to its costs and hardware requirements, tend to be orchestrated by very centralized entities. This brings a major
restriction to the adoption of these technologies to automatize a larger number of applications of society sectors. Indeed, it is
expected that large institutions will not be happy to share their internal data with OpenAI, or other big tech companies.

For this reason, we propose a different methodology, that of a `Context Driven Knowledge` protocols (CDKs). LLMs habe proved to be
suitable for problems akin to transform and extract contents from text into a more structured representations. The latter can be either
in the form of Knowledge Graphs (KGs), Code programs (see Voyager paper), or even Named Entity Relations, etc. The later form
data structures (very different in nature to usual generators). The main advantage of these later representations is that these can 
be more easily coordinated across networks and they satisfy a `locality` condition. That is, in order to have acess to specific chunks
of knowledge, we don't need to deal with terabytes of global parameters (like in LLMs).

Moreover, such structures are suitable for LLM interaction. Indeed, they provide natural frameworks for fine tuning of LLMs, etc. 

### Design

We propose a simple framework to protocolize a network of Knowledge Graphs, boosted by the powers of LLMs.


### Start service

The first step is to have a running neo4j database container running.
In order to do so, please run

`$ docker compose up`

To start the http service, you need to run

`$ cd bin http_server & cargo run`

This will start the HTTP server, with default port `3000`. You can start making requests to the root, via Postman or you preferred tool. A basic 
post request to the service, looks like:

````
{
    "chunk": "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 37, 8]. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to understand common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be aware of a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the modelâ€™s desired responses and behavior from its very wide knowledge and abilities is crucial to building AI systems that are safe, performant, and controllable [23]. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline. At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12, 2]). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs. In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, without explicitly learning a reward function or sampling from the policy during training. Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.",
    "model": "gpt-4",
    "max_tokens": 5000,
    "temperature": 0.7
}
```

To check the resulting state of the Neo4j database, you can open `localhost:7474` on your browser and check the generated (Knowledge) Graph.
